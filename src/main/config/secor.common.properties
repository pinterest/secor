# Licensed to the Apache Software Foundation (ASF) under one or more
# contributor license agreements.  See the NOTICE file distributed with
# this work for additional information regarding copyright ownership.
# The ASF licenses this file to You under the Apache License, Version 2.0
# (the "License"); you may not use this file except in compliance with
# the License.  You may obtain a copy of the License at
#
#    http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.

############
# MUST SET #
############

# Regular expression matching names of consumed topics.
secor.kafka.topic_filter=.*

# AWS authentication credentials.
aws.access.key=
aws.secret.key=

################
# END MUST SET #
################

# AWS region or endpoint. region should be a known region name (eg.
# us-east-1). endpoint should be a known S3 endpoint url. If neither
# are specified, then the default region (us-east-1) is used. If both
# are specified then endpoint is used.
#
# Only apply if the the S3UploadManager is used - see
# secor.upload.manager.class.
#
# http://docs.aws.amazon.com/general/latest/gr/rande.html#s3_region
aws.region=
aws.endpoint=

# Zookeeper config.
zookeeper.session.timeout.ms=3000
zookeeper.sync.time.ms=200

# Zookeeper path (chroot) under which secor data will be placed.
secor.zookeeper.path=/

# Impacts how frequently the upload logic is triggered if no messages are delivered.
kafka.consumer.timeout.ms=10000

# Max number of retries during rebalance.
kafka.rebalance.max.retries=

# Rebalance backoff.
kafka.rebalance.backoff.ms=

# Kafka consumer receive buffer size (socket.receive.buffer.bytes)
kafka.socket.receive.buffer.bytes=

# Kafka fetch max size (fetch.message.max.bytes)
kafka.fetch.message.max.bytes=

# Kafka fetch min bytes (fetch.fetch.min.bytes)
kafka.fetch.min.bytes=

# Kafka fetch max wait ms (fetch.max.wait.ms)
kafka.fetch.wait.max.ms=

# Port of the broker serving topic partition metadata.
kafka.seed.broker.port=9092

# Zookeeper path at which kafka is registered. In Zookeeper parlance, this is referred
# to as the chroot.
kafka.zookeeper.path=/

# Secor generation is a version that should be incremented during non-backwards-compabile
# Secor releases. Generation number is one of the components of generated log file names.
# Generation number makes sure that outputs of different Secor versions are isolated.
secor.generation=1

# Number of consumer threads per Secor process.
secor.consumer.threads=7

# Consumption rate limit enforced at the process level (not a consumer-thread level).
secor.messages.per.second=10000

# Used by the "backup" consumer group only.
# Number of continous message offsets that constitute a single offset= partition on s3.
# Example:
#   if set to 10,
#     messages with offsets 0 to 9 will be written to s3 path s3n://.../offset=0/...
#     messages with offsets 10 to 19 will be written to s3 path s3n://.../offset=10/...
#     ...
secor.offsets.per.partition=10000000

# How long does it take for secor to forget a topic partition. Applies to stats generation only.
secor.topic_partition.forget.seconds=600

# Setting the partitioner to use hourly partition
# By default, the partitioner will do daily partition, so the data will be 
# written into 
#       s3n://.../topic/dt=2015-07-07/
# If this parameter is set to true, the data will be written into
#       s3n://.../topic/dt=2015-07-07/hr=02
# The hour folder ranges from 00 to 23
# partitioner.granularity.hour=true

# During partition finalization, the finalizer will start from the last
# time partition (e.g. dt=2015-07-17) and traverse backwards for n 
# partition periods (e.g. dt=2015-07-16, dt=2015-07-15 ...)
# This parameter controls how many partition periods to traverse back
# The default is 10
# secor.finalizer.lookback.periods=10

# If greater than 0, upon starup Secor will clean up directories and files under secor.local.path
# that are older than this value.
secor.local.log.delete.age.hours=-1

# Secor comes with a tool that adds Hive partitions for finalized topics. Currently, we support
# only Hive clusters accessible through Qubole. The token gives access to the Qubole API.
# It is available at https://api.qubole.com/users/edit
qubole.api.token=

# hive tables are generally named after the topics. For instance if the topic is request_log
# the hive table is also called request_log. If you want this to be pinlog_request_log you can
# set this config to "pinlog_". This affects all topics.
hive.table.prefix=

# Secor can export stats such as consumption lag (in seconds and offsets) per topic partition.
# Leave empty to disable this functionality.
tsdb.hostport=

# Regex of topics that are not exported to TSDB.
monitoring.blacklist.topics=

# Prefix of exported statss.
monitoring.prefix=secor

# Secor can export stats to statsd such as consumption lag (in seconds and offsets) per topic partition.
# Leave empty to disable this functionality.
statsd.hostport=

# Name of field that contains timestamp for JSON, MessagePack, or Thrift message parser. (1405970352123)
message.timestamp.name=timestamp

# Name of field that contains a timestamp, as a date Format, for JSON. (2014-08-07, Jul 23 02:16:57 2005, etc...)
# Should be used when there is no timestamp in a Long format. Also ignore time zones.
message.timestamp.input.pattern=

# To enable compression, set this to a valid compression codec implementing 
# org.apache.hadoop.io.compress.CompressionCodec interface, such as 
# 'org.apache.hadoop.io.compress.GzipCodec'.
secor.compression.codec=

# The secor file reader/writer used to read/write the data, by default we write sequence files
secor.file.reader.writer.factory=com.pinterest.secor.io.impl.SequenceFileReaderWriterFactory

# Max message size in bytes to retrieve via KafkaClient. This is used by ProgressMonitor and PartitionFinalizer.
# This should be set large enough to accept the max message size configured in your kafka broker
# Default is 0.1 MB
secor.max.message.size.bytes=100000

# Class that will manage uploads. Default is to use the hadoop
# interface to S3.
secor.upload.manager.class=com.pinterest.secor.uploader.HadoopS3UploadManager
