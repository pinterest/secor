If secor is deployed the first time or if deploying another secor with a 
NEW "secor.group.id" (It will create a new node in zk "/consumers") There is 
a possibilty that secor will run out of disk space.  It is because secor will
pull all the data that kafka has been keeping (~60 days) and store them in 
"/tmp/secor_backup".

Currently secor is using "tr.small" and has 8G storage, it is not enough for a
production environment with lots of data daily.

===============================================================================
If secor keeps fetching metadata for some topics, there will be a lot of this
kind of messages in the karyon.log

2017-04-20 18:14:02 INFO  ClientUtils$:68 - Fetching metadata from broker id:5,host:ip-172-30-198-228.us-west-2.compute.internal,port:9092 with correlation id 290122 for 1 topic(s) Set(my-topic)
2017-04-20 18:14:02 INFO  SyncProducer:68 - Disconnecting from ip-172-30-199-12.us-west-2.compute.internal:9092
2017-04-20 18:14:02 INFO  ConsumerFetcherManager:68 - [ConsumerFetcherManager-1492644371208] Added fetcher for partitions ArrayBuffer()
2017-04-20 18:14:02 INFO  SyncProducer:68 - Connected to ip-172-30-198-228.us-west-2.compute.internal:9092 for producing
2017-04-20 18:14:02 INFO  SyncProducer:68 - Disconnecting from ip-172-30-198-228.us-west-2.compute.internal:9092
2017-04-20 18:14:02 INFO  ConsumerFetcherManager:68 - [ConsumerFetcherManager-1492644371207] Added fetcher for partitions ArrayBuffer()
2017-04-20 18:14:02 INFO  SyncProducer:68 - Disconnecting from ip-172-30-198-228.us-west-2.compute.internal:9092
2017-04-20 18:14:02 INFO  ConsumerFetcherManager:68 - [ConsumerFetcherManager-1492644371215] Added fetcher for partitions ArrayBuffer()
2017-04-20 18:14:02 INFO  VerifiableProperties:68 - Verifying properties
2017-04-20 18:14:02 INFO  VerifiableProperties:68 - Property client.id is overridden to secor_backup
2017-04-20 18:14:02 INFO  VerifiableProperties:68 - Property metadata.broker.list is overridden to ip-172-30-199-12.us-west-2.compute.internal:9092,ip-172-30-198-228.us-west-2.compute.internal:9092,ip-172-30-198-98.us-west-2.compute.internal:9092

1. Check if the DNS of the kafka brokers are correct
2. Log into zookeeper and check the topic and its broker ids, for example, stable kafka has broker ids 4, 5, and 6
This is from a bad topic in stable kafka

From dev bastion,
    ssh -i ~/.ssh/main_access.pem ec2-user@zookeeper.us-west-2.stable.dev.oneplatform.build
    /opt/reuters/apps/zookeeper/bin/zkCli.sh

[zk: localhost:2181(CONNECTED) 6] get /brokers/topics/my-topic

{"version":1,"partitions":{"45":[4,3],"98":[3,1],"34":[5,2],"67":[2,6],"93":[4,1],"12":[1,6],"66":[1,5],"89":[6,2],"51":[4,5],"84":[1,3],"8":[3,1],"73":[2,1],"78":[1,2],"19":[2,3],"23":[6,1],"62":[3,6],"4":[5,2],"88":[5,1],"77":[6,5],"40":[5,3],"15":[4,3],"11":[6,4],"90":[1,4],"9":[4,2],"44":[3,2],"33":[4,1],"22":[5,6],"56":[3,5],"55":[2,4],"26":[3,5],"50":[3,4],"37":[2,6],"68":[3,1],"61":[2,5],"13":[2,1],"46":[5,4],"99":[4,2],"24":[1,3],"94":[5,2],"83":[6,1],"35":[6,3],"16":[5,4],"79":[2,3],"5":[6,3],"72":[1,6],"10":[5,3],"59":[6,2],"87":[4,6],"48":[1,2],"21":[4,5],"76":[5,4],"54":[1,3],"43":[2,1],"65":[6,3],"71":[6,4],"57":[4,6],"32":[3,6],"80":[3,4],"82":[5,6],"49":[2,3],"6":[1,5],"36":[1,5],"1":[2,5],"39":[4,2],"17":[6,5],"25":[2,4],"60":[1,4],"14":[3,2],"47":[6,5],"31":[2,5],"96":[1,5],"69":[4,2],"95":[6,3],"58":[5,1],"64":[5,2],"53":[6,1],"42":[1,6],"75":[4,3],"0":[1,4],"20":[3,4],"27":[4,6],"70":[5,3],"2":[3,6],"86":[3,5],"38":[3,1],"81":[4,5],"92":[3,6],"18":[1,2],"30":[1,4],"7":[2,6],"97":[2,6],"29":[6,2],"41":[6,4],"63":[4,1],"3":[4,1],"74":[3,2],"91":[2,5],"52":[5,6],"85":[2,4],"28":[5,1]}}

It shows that some partitions are tied to broker 1, 2, or 3, which is not correct. Hence it is more like #1 because those brokers can't be find.

========================================================================================

To build in jenkins, selected "gradlew" and set switches to "-x generateProto -x compileTestJava" to bypass protoc, which is not installed
on jenkins.

for stable : use task "jacocoTestCoverageVerification" instead of "jacoco"

checked "Force GRADLE_USER_HOME to use workspace"
